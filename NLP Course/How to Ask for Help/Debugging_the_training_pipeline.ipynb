{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAw7eoJlWBzi"
      },
      "source": [
        "# Debugging the training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5L1-maiWBzk"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po7Lv58WWBzk"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwL7qaH4WBzk",
        "outputId": "f440d019-4969-451e-809f-994b4440fa88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ValueError: You have to specify either input_ids or inputs_embeds'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=raw_datasets[\"train\"],\n",
        "    eval_dataset=raw_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYjAbpxEWBzl",
        "outputId": "946ea6dd-d741-4fbb-fe72-7cb0c27f1885"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
              " 'idx': 0,\n",
              " 'label': 1,\n",
              " 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH2LZMYIWBzl",
        "outputId": "b7fdc308-0741-4e78-c9d0-0f1f1e629cc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ValueError: expected sequence of length 43 at dim 1 (got 37)'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgILY-zaWBzm",
        "outputId": "08e14290-8fd6-48c3-96fa-395ce2180987"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHd6DeQ9WBzm",
        "outputId": "56172e79-68e6-4721-d04a-24f814257e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGmQa1P4WBzm",
        "outputId": "bbcaa78b-cfb8-4a7e-f824-b6697a70f1b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIOmk2iiWBzm",
        "outputId": "01994bc1-26ae-40f6-9dcd-48be728db372"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0][\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrr_L84wWBzm",
        "outputId": "b257353d-eed6-4608-d43c-fb4c1acfe23b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trainer.train_dataset[0][\"attention_mask\"]) == len(\n",
        "    trainer.train_dataset[0][\"input_ids\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s07WasEiWBzn",
        "outputId": "2f8cd1d2-21d4-4835-cd0a-65214140bfa8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0][\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3usQAZ4WBzn",
        "outputId": "0b79dfd0-fc7d-4cf7-b26d-68733499c2c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['entailment', 'neutral', 'contradiction']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset.features[\"label\"].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RRhh9X5WBzn",
        "outputId": "8dc227b0-a278-42b7-8821-3c5c629b384e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)\n",
              "    105                 batch[k] = torch.stack([f[k] for f in features])\n",
              "    106             else:\n",
              "--> 107                 batch[k] = torch.tensor([f[k] for f in features])\n",
              "    108 \n",
              "    109     return batch\n",
              "\n",
              "ValueError: expected sequence of length 45 at dim 1 (got 76)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59vJr3YNWBzn",
        "outputId": "469cbfe8-3470-49c8-8554-60ce61a4d812"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeZPcoQRWBzn",
        "outputId": "4784df29-be2d-4acb-fd3f-82a230ad3283"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qmL8hv6WBzn"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "batch = data_collator([trainer.train_dataset[i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pOysSGvWBzn"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)\n",
        "batch = data_collator([actual_train_set[i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgJaMfaCWBzn"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-ZhoyqYWBzn",
        "outputId": "bfa1fa75-73c2-4843-e6e0-8c19ef101b02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n",
              "   2386         )\n",
              "   2387     if dim == 2:\n",
              "-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
              "   2389     elif dim == 4:\n",
              "   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
              "\n",
              "IndexError: Target 2 is out of bounds."
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = trainer.model.cpu()(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "885FKGRkWBzn",
        "outputId": "4a37e2b9-d4e9-435b-949a-c892e9d64323"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.config.num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxZcSXo0WBzo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNsvjsaJWBzo"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break\n",
        "\n",
        "outputs = trainer.model.cpu()(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hFMJLBbWBzo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "outputs = trainer.model.to(device)(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc8BmYxyWBzo"
      },
      "outputs": [],
      "source": [
        "loss = outputs.loss\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydtOrR1KWBzo"
      },
      "outputs": [],
      "source": [
        "trainer.create_optimizer()\n",
        "trainer.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PiJ75sFWBzo",
        "outputId": "1f80f55a-058f-48fd-cd77-ca43b5da5b02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This will take a long time and error out, so you shouldn't run this cell\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJIJmbyXWBzo",
        "outputId": "8ae74d4b-7ee5-437e-a612-ae6a33421fe3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grI6bmXDWBzo"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trainer.model(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLj2ZdHGWBzo",
        "outputId": "a6cbdd5d-df7f-41fe-c700-38d6202c6ee6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = outputs.logits.cpu().numpy()\n",
        "labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "compute_metrics((predictions, labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAP-GYslWBzo",
        "outputId": "7487bbee-5c92-482e-d040-308d8674cf7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8, 3), (8,))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTIKhomvWBzo",
        "outputId": "f6f67258-194e-4f01-8169-e013f67798b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.625}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "compute_metrics((predictions, labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhIDm8SEWBzo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4MZjOvNWBzo"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break\n",
        "\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "trainer.create_optimizer()\n",
        "\n",
        "for _ in range(20):\n",
        "    outputs = trainer.model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    trainer.optimizer.step()\n",
        "    trainer.optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx3u4e93WBzo",
        "outputId": "d46a0f3f-f555-407c-d4f4-1f208aad8c08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 1.0}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = trainer.model(**batch)\n",
        "preds = outputs.logits\n",
        "labels = batch[\"labels\"]\n",
        "\n",
        "compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Debugging the training pipeline",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}